{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890b2f1-f31a-4a79-96b0-7e52bf8bdb12",
   "metadata": {},
   "source": [
    "## 0: Langchain basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a472f-34eb-4a0b-af32-57dab0e470e0",
   "metadata": {},
   "source": [
    "Make sure you are using the python binary in the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f308cd-878a-466d-b4d1-e40253bbbead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cesargmx/llm-workshop/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ae306-8c6c-433f-b8a4-d76884a26ccb",
   "metadata": {},
   "source": [
    "### Basic LLM Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0c96b3-a3d3-4620-be21-bcf56256b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that gets to the heart of many a fantasy fan's passion!\n",
      "\n",
      "The author of the A Song of Ice and Fire series is George R.R. Martin. The series, which includes seven novels (so far!), has been adapted into the hit HBO television show Game of Thrones.\n",
      "\n",
      "George R.R. Martin is an American novelist, screenwriter, and television producer. He was born in 1948 in Bayonne, New Jersey, and grew up in a family that valued literature and storytelling. Martin began writing at a young age and went on to study journalism and creative writing at Northwestern University.\n",
      "\n",
      "The A Song of Ice and Fire series is Martin's most famous work, and it has become a cultural phenomenon. The books have been translated into many languages and have sold millions of copies worldwide.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "response = llm.invoke(\"who wrote A Song of Ice and Fire?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e9346-4a6f-4275-9da5-22483de686f1",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "LLMs can be combined with other components, such as external data sources or other LLMs, to create more complex applications.\n",
    "\n",
    "A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, LLMs, utils, or other chains.\n",
    "\n",
    "You may find more information in the official documentation: https://python.langchain.com/v0.1/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018649e-abd8-4e8a-a89d-e140214a7341",
   "metadata": {},
   "source": [
    "#### Plain text output, string template prompt, invoke vs stream\n",
    "\n",
    "First we instantiate the llama3 model. We need to explain the parameters, and what they do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3413ffe7-1ea7-4a5a-9934-8eba7175cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\", \n",
    "    keep_alive=-1,\n",
    "    \n",
    "    # You can experiment with the following parameters:\n",
    "    temperature=0,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813b8a6-e829-4c7d-a01f-dfa51b820992",
   "metadata": {},
   "source": [
    "Here we create a prompt from a string template, and the chain. Notice this line of the code, where we piece together these different components into a single chain using LCEL:\n",
    "\n",
    "```\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "The `|` symbol is similar to a [unix pipe operator](https://en.wikipedia.org/wiki/Pipeline_(Unix)), which chains together the different components, feeding the output from one component as input into the next component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843a6a65-159a-448b-8a46-eaaf54eb6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write me the lyrics for a 30 seconds jingle about {product}\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898636c8-5989-43cd-82d7-f6f900458f36",
   "metadata": {},
   "source": [
    "When we execute the chain, we need to pass the parameters required by the prompt. Note that if we execute the chain with `invoke` all the text is presented after it was fully generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e974e257-0618-4144-aaf0-c7e248848210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 30-second jingle for home insurance:\n",
      "\n",
      "(Upbeat, catchy tune)\n",
      "\"Home sweet home, where memories grow\n",
      "Protect it with care, don't you know?\n",
      "From roofs to walls, to floors and more\n",
      "We've got you covered, that's what we're looking for!\n",
      "\n",
      "Our home insurance is the best\n",
      "For your peace of mind, we pass the test\n",
      "So why wait? Get a quote today\n",
      "And keep your home safe in every way!\"\n",
      "\n",
      "This jingle aims to be catchy and easy to remember, while also highlighting the importance of protecting one's home. The lyrics are short, simple, and easy to sing along to, making it perfect for a 30-second radio ad or TV commercial.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"product\": \"home insurance\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37123b2-7843-4b59-85e6-6f58bc837531",
   "metadata": {},
   "source": [
    "Here we execute the chain with `stream`. Notice how we can update the output after each token is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c466e6bf-d202-48e1-815b-f5e742aca3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 30-second jingle for cat food:\n",
      "\n",
      "(Upbeat, catchy tune)\n",
      "\"Meow, meow, it's time to know\n",
      "The best food for your kitty to go!\n",
      "Whisker Delight, crunchy and fine\n",
      "Makes your feline friend feel so divine!\n",
      "\n",
      "Purr-fectly tasty, every single bite\n",
      "Whisker Delight, the cat food that's right!\n",
      "So serve it up, and watch them play\n",
      "With Whisker Delight, every day!\"\n",
      "\n",
      "(End of jingle)\n",
      "\n",
      "This jingle is designed to be short, catchy, and easy to remember. The lyrics highlight the key benefits of the cat food (crunchy texture, tasty flavor) and emphasize its appeal to cats. The melody is upbeat and energetic, making it perfect for a 30-second radio ad or TV commercial."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"product\": \"cat food\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcd487-5286-46e5-94f4-7c680ffc8c17",
   "metadata": {},
   "source": [
    "#### Json Output, message based prompt\n",
    "\n",
    "A Json output ensures we can use the output as input for other services/models. We can also create a prompt via messages. This is useful for chat based agents. \n",
    "\n",
    "First we  define the output schema and instantiate the model, specifying json as output format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cff3d8-c48b-4d29-bb54-357fe222407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_schema = {\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "        \"favorite_food\": {\n",
    "            \"title\": \"Fav Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"],\n",
    "}\n",
    "\n",
    "output_schema_as_string = json.dumps(output_schema, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5c332d-86c9-4687-acc1-a8a7bec88474",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    format=\"json\",\n",
    "    keep_alive=-1, # keep the model loaded indefinitely\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6184c8-d081-4bed-a79c-c92080029ac6",
   "metadata": {},
   "source": [
    "We create the prompt with a list of messages. Each message is associated with content, and an additional parameter called `role`. For example, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6386996f-6c93-4aaf-93f8-51c51ef22601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "messages = [\n",
    "    (\"system\", \n",
    "         \"You are a helpful assistant that will extract information about a person and produce \"\n",
    "         \"an output using the following json schema: {json_schema}\"),\n",
    "    (\"human\", \"{person_info}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb88fab-f1f7-44c0-a469-4a946f683ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Cesar', 'age': 37, 'favorite_food': 'sushi'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"json_schema\": output_schema_as_string,\n",
    "        \"person_info\": \"Cesar is 37 and loves sushi\"\n",
    "    })\n",
    "\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb97d1c-8a7b-4a11-8e04-f3b8fb021db0",
   "metadata": {},
   "source": [
    "Additionally, LangChain provides different types of `MessagePromptTemplate`. The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively. You can read more about the different type of messages [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/message_types/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee12aac-e6f7-46c1-97e6-c1e03cea2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "         \"You are a helpful assistant that will extract information about a person and produce \"\n",
    "         \"an output using the following json schema: {json_schema}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{person_info}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffdfa9-6caf-4cbd-b07b-3b06c01ee6fe",
   "metadata": {},
   "source": [
    "Notice how if we change the output parser, we get the same result, but the object is a string instead of a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87fd75ff-af0c-4895-afa1-423f9b44b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"Name\": \"Cesar\",\n",
      "\"Age\": 37,\n",
      "\"Fav Food\": \"sushi\"\n",
      "}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "chain_str_parser = prompt | llm | StrOutputParser()\n",
    "response = chain_str_parser.invoke(\n",
    "    {\n",
    "        \"json_schema\": output_schema_as_string,\n",
    "        \"person_info\": \"Cesar is 37 and loves sushi\"\n",
    "    })\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffafbd3-340b-4e51-9a6d-57a4f8bcce4f",
   "metadata": {},
   "source": [
    "The power from LLMs is that it can be used to extract information from natural language and produce an output in a format that can be used for integrations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c79c15-b441-4e8b-8699-b92197048e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Laura', 'age': 30, 'favorite_food': 'decadent chocolate lava cake'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_info_text = \"\"\"As Laura walked into the cozy café, her bright smile illuminated the room, and \n",
    "her sparkling eyes seemed to dance with a youthful energy. Her long, curly brown hair bounced with \n",
    "each step, framing her heart-shaped face and emphasizing her petite nose ring. She was dressed in \n",
    "a flowy sundress, its vibrant floral pattern reflecting her playful personality. As she scanned the \n",
    "menu, her eyes widened with excitement, and she couldn't help but let out a little squeal of delight\n",
    "when she spotted her favorite dish: a decadent chocolate lava cake. She had always been a \n",
    "self-proclaimed chocoholic, and the mere mention of the rich, gooey treat was enough to transport \n",
    "her back to her childhood days of baking with her grandmother. With a spring in her step, she \n",
    "ordered her beloved dessert and settled in for a delightful afternoon of indulgence, to celebrate  \n",
    "her 30th birthday.\"\"\"\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"json_schema\": output_schema_as_string,\n",
    "        \"person_info\": person_info_text\n",
    "    })\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a311b54-d3ce-4396-8713-e9a10dc2d61a",
   "metadata": {},
   "source": [
    "#### Chaining two prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f432d0f-70f0-4c1a-99c8-b8a56fa6d005",
   "metadata": {},
   "source": [
    "In this example we use the output of a LLM prompt as the input for another. There are two approaches, the first one is with LCEL and the second one with SimpleSequentialChain.\n",
    "\n",
    "##### Using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8964887f-1ca7-491e-b71d-eadc734b0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a simple recipe to make Carbonara at home:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 12 oz spaghetti\n",
      "* 6 slices of pancetta or bacon, diced\n",
      "* 2 large eggs\n",
      "* 1/2 cup grated Parmesan cheese\n",
      "* Salt and black pepper, to taste\n",
      "* Fresh parsley, chopped (optional)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. Bring a large pot of salted water to a boil. Cook the spaghetti according to package instructions until al dente.\n",
      "2. While the pasta is cooking, cook the pancetta or bacon in a skillet over medium heat until crispy.\n",
      "3. In a separate bowl, whisk together the eggs and Parmesan cheese.\n",
      "4. When the spaghetti is done, reserve 1 cup of pasta water before draining.\n",
      "5. Add the cooked spaghetti to the bowl with the egg mixture. Toss everything together, adding some reserved pasta water if needed to create a creamy sauce.\n",
      "6. Add the crispy pancetta or bacon to the bowl and toss again to combine.\n",
      "7. Season with salt and black pepper to taste.\n",
      "8. Serve immediately, garnished with chopped parsley if desired.\n",
      "\n",
      "Enjoy your homemade Carbonara!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION: {user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_location = ChatPromptTemplate.from_template(template)\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL: {user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_meal = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt_location | llm | prompt_meal | llm | output_parser\n",
    "\n",
    "response = chain.invoke(\"Rome\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a7d05-afc5-4642-a423-a853c5bd0583",
   "metadata": {},
   "source": [
    "##### Using SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce1cf33-763b-4da0-a90e-961db0f40da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cesargmx/llm-workshop/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/Users/cesargmx/llm-workshop/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRome!\n",
      "\n",
      "In that case, I'd like to suggest a classic Roman dish that's both iconic and delicious: Carbonara.\n",
      "\n",
      "Carbonara is a rich and creamy pasta dish made with spaghetti, bacon or pancetta, eggs, parmesan cheese, and black pepper. It's a staple of Roman cuisine and a favorite among locals and tourists alike.\n",
      "\n",
      "Legend has it that the name \"carbonara\" comes from the Italian word for \"coal miner,\" as this hearty dish was originally created to fuel the working-class coal miners in Rome. Whatever its origins, Carbonara is a true Roman classic that's sure to satisfy your taste buds!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mA delicious and iconic Roman dish! Here's a simple recipe to make Carbonara at home:\n",
      "\n",
      "**Serves 4**\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 12 oz spaghetti\n",
      "* 6 slices of bacon or pancetta, diced\n",
      "* 3 large eggs\n",
      "* 1/2 cup grated Parmesan cheese\n",
      "* Salt and black pepper, to taste\n",
      "* Fresh parsley, chopped (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Bring a large pot of salted water to a boil. Cook the spaghetti according to package instructions until al dente. Reserve 1 cup of pasta water before draining.\n",
      "2. In a large skillet, cook the diced bacon or pancetta over medium heat until crispy. Remove from heat and set aside.\n",
      "3. In a medium bowl, whisk together the eggs, Parmesan cheese, and a pinch of salt and pepper.\n",
      "4. Add the cooked spaghetti to the egg mixture and toss until well combined.\n",
      "5. Add the reserved pasta water in small increments if the mixture seems too thick.\n",
      "6. Add the cooked bacon or pancetta to the spaghetti mixture and toss to combine.\n",
      "7. Season with salt and black pepper to taste.\n",
      "8. Serve immediately, garnished with chopped parsley if desired.\n",
      "\n",
      "Enjoy your homemade Carbonara!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION: {user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Holds the'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds the'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "\n",
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d034f-94d7-4940-ab92-98f954ffe336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8b648-8c1c-45b6-ae1b-c41238c2bbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d21b5d6-04b6-4510-acef-b1f04097b0e4",
   "metadata": {},
   "source": [
    "#### Structured Text Prompt and Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bfaf8-6f3e-4978-a923-67e97a7e9bd7",
   "metadata": {},
   "source": [
    "We can go one step further and create our prompt using [Llama 3's prompt format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/). See an example below: \n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for travel tips and recommendations<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What can you help me with?<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "\n",
    "**<|begin_of_text|>**: Specifies the start of the prompt\n",
    "\n",
    "**<|start_header_id|>system<|end_header_id|>**: Specifies the role for the following message, i.e. “system”\n",
    "\n",
    "**You are a helpful AI assistant for travel tips and recommendations**: The system message\n",
    "\n",
    "**<|eot_id|>**: Specifies the end of the input message\n",
    "\n",
    "**<|start_header_id|>user<|end_header_id|>**: Specifies the role for the following message i.e. “user”\n",
    "\n",
    "**What can you help me with?**: The user message\n",
    "\n",
    "**<|start_header_id|>assistant<|end_header_id|>**: Ends with the assistant header, to prompt the model to start generation.\n",
    "\n",
    "Following this prompt, Llama 3 completes it by generating the {{assistant_message}}. It signals the end of the {{assistant_message}} by generating the **<|eot_id|>**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aecf6404-2e2b-4bca-a34f-9f912056f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# Pydantic Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    age: float = Field(description=\"The person's age\", required=True)\n",
    "    favorite_food: str = Field(description=\"The person's favorite food\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5148b8c-a9cb-4bd3-9e74-6dcbd8eb0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template llama3\n",
    "prompt = PromptTemplate.from_template(\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a smart assistant take the following context and question below and return your answer in JSON.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "QUESTION: {question} \\n\n",
    "CONTEXT: {context} \\n\n",
    "JSON:\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm = OllamaFunctions(model=\"llama3\", \n",
    "                      format=\"json\", \n",
    "                      temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    "chain = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6781f41-6900-4293-b20b-cbaee50ceba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Emma', age=8.0, favorite_food='Pizza')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\"Three friends are celebrating their birthday together, eating their favorite food at Pizza Hut. \n",
    "Mia is 2 years older than Jake, and Emma is 2 years younger than Jake. Jake is 10 years old.  \"\"\"\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"question\": \"How old is Emma?\",\n",
    "    \"context\": context\n",
    "    })\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a2613-eb19-45f6-8cb0-60bec5ec7450",
   "metadata": {},
   "source": [
    "### Functions / Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651804b9-6c2b-4346-ad3f-5f669764db67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1bab7-2170-4312-97c9-e07f3e603010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54146463-6947-410e-a05b-6c700628f3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
